
InhaltsverzeichnisARBEITSTITEL	31.1 EINLEITUNG	31.2 PROBLEMDIMENSIONEN	31.3 ZIELSETZUNG DER ARBEIT UND LEITFRAGEN	61.4 METHODIK	71.4.1 Forschungsmethode	71.4.2 Technische Konstruktionsmethoden	72.1 MOTIVATION	82.2 UNTERNEHMENSAKZEPTANZ	92.3 ZEITPLAN	102.4 GROBGLIEDERUNG	102..4 ARTIFACT CONSTRUCTION: METHODEN, PRINZIPIEN UND WERKZEUGE (ENTWURF)	12LITERATURVERZEICHNIS (EXPOSÉ)	13
Arbeitstitel„Entwicklung einer Enterprise-Referenzarchitektur für Generative KI: Automatisierte Quality Gates und EU-AI-Act-Compliance in cloud-nativen Betriebsumgebungen"1.1 EinleitungDie rasante Adaption generativer KI (GenAI) und Large Language Models (LLMs) stellt Unternehmen vor operative Herausforderungen, die über den Wirkungsbereich etablierter MLOps-Verfahren hinausgehen. Bestehende CI/CD-Pipelines decken die spezifischen Risiken stochastischer Modelle — etwa Halluzinationen, Promptabhängigkeit oder kontextabhängige Qualitätsschwankungen — nicht systematisch ab. Der produktive Betrieb generativer KI-Systeme, insbesondere großer Sprachmodelle und darauf aufbauender Retrieval-Augmented-Generation-(RAG) Architekturen, stellt Unternehmen damit vor qualitativ neue Herausforderungen, die mit etablierten MLOps-Frameworks nur unzureichend adressiert werden können (Sinha et al., 2024). Dieser Befund lässt sich entlang dreier interdependenter Problemdimensionen konkretisieren.1.2 ProblemdimensionenPD1: Fragmentierung bestehender Operationalisierungs-frameworksTraditionelle MLOps-Ansätze wurden für diskriminative Modelle mit stabilen, strukturierten Datenpipelines konzipiert. Generative KI-Systeme unterscheiden sich fundamental durch probabilistisches Ausgabeverhalten, stochastische Qualitätsschwankungen, Promptabhängigkeit sowie erhöhte Anforderungen an Observability und Sicherheitsbewertung (Sinha et al., 2024, S. 1–2). Bestehende Frameworks adressieren dabei typischerweise isolierte Lebenszyklusphasen — etwa Training, Deployment oder Monitoring —, ohne eine integrierte End-to-End-Governance zu ermöglichen (Wang et al., 2025, S. 1; Díaz-De-Arcaya et al., 2024, S. 3). Die Konsequenz dieser Fragmentierung ist in der unternehmerischen Praxis messbar: Stone et al. berichten, dass 55 % der befragten Unternehmen keine produktiven Modelle betreiben und 18 % mehr als 90 Tage für die Bereitstellung benötigen (Stone et al., 2025, S. 1).Obwohl sich die Operationsparadigmen von MLOps über LLMOps zu GenAIOps evolutionär weiterentwickeln (Kreuzberger et al., 2023; Pahune & Akhtar, 2025; Joshi, 2025; Tantithamthavorn et al., 2025), ist GenAIOps als operativer Rahmen bislang weder konzeptuell konsolidiert noch in eine standardisierte Enterprise-Referenzarchitektur überführt worden (Joshi, 2025, S. 8; Pahune & Akhtar, 2025, S. 23).Diese Lifecycle-Fragmentierung hat unmittelbare Konsequenzen für die Qualitätssicherung: Wo kein integriertes Framework die Phasenübergänge strukturiert, fehlt auch die Grundlage für phasenübergreifende Qualitätskontrollmechanismen — ein Defizit, das die folgende Problemdimension vertieft.PD2: Methodische Inadäquanz bestehender Qualitätssicherungsmechanismen für generative KI-SystemeDie in Problemdimension 1 beschriebene Fragmentierung wird durch eine methodische Lücke in der Qualitätssicherung verschärft: Etablierte Verfahren zur Qualitätskontrolle in MLOps-Pipelines basieren auf deterministischen Metriken wie Accuracy, F1-Score oder AUC-ROC, die für diskriminative Modelle mit stabilen Ein-/Ausgabebeziehungen konzipiert wurden (Polyakovska, 2025, S. 5). Generative KI-Systeme erzeugen jedoch stochastische, kontextabhängige Outputs, deren Qualität nicht mit einzelnen numerischen Schwellenwerten erfasst werden kann (Polyakovska, 2025, S. 2). Diese methodische Inadäquanz manifestiert sich in drei konkreten Defiziten.Erstens fehlen standardisierte Evaluationsmetriken für GenAI-spezifische Risikodimensionen. Ohne messbare, reproduzierbare Kriterien für Retrieval-Precision, Generierungsqualität, Halluzinationsraten oder Prompt-Injection-Vulnerabilities können keine automatisierten Schwellenwerte definiert werden, an denen Freigabe- oder Ablehnungsentscheidungen systematisch getroffen werden (Xu et al., 2025, S. 14). Zweitens ist die Observability-Infrastruktur für generative KI-Systeme unzureichend entwickelt: Jede formalisierte Qualitätskontrolle setzt nachprüfbare Evidenz voraus, doch die infrastrukturellen Grundlagen zur Erzeugung und Persistierung dieser Evidenz fehlen weitgehend (Xu et al., 2025, S. 15; Polyakovska, 2025, S. 6–7). Drittens fehlt eine phasenübergreifende Orchestrierung von Qualitätskontrollpunkten über den GenAI-Lebenszyklus: Qualitätsprüfungen finden isoliert innerhalb einzelner Phasen statt, ohne definierte Abhängigkeiten oder Übergabekriterien zwischen den Phasen (Wang et al., 2025, S. 1–2).Diese drei Defizite beschreiben zusammengenommen eine strukturelle Lücke: Für generative KI-Systeme existieren keine formalisierten, automatisierbaren Kontrollpunkte, die den Übergang zwischen Lebenszyklusphasen an prüfbare Qualitätskriterien und nachweisbare Evidenz binden. Damit fehlt eine zentrale Voraussetzung sowohl für die technische Qualitätssicherung als auch für die regulatorische Nachweisführung, die Gegenstand der dritten Problemdimension ist.PD3: Compliance-Operationalisierungslücke (EU AI Act)Selbst wenn ein integriertes GenAI-Lifecycle-Framework (PD1) sowie belastbare Qualitätsmethoden (PD2) verfügbar wären, bleibt die regulatorische Nachweisbarkeit als eigenständige Engstelle bestehen. Der EU AI Act definiert für Hochrisiko-KI-Systeme verbindliche Anforderungen an Risikomanagement (Art. 9), Daten-Governance (Art. 10), technische Dokumentation (Art. 11), Protokollierung (Art. 12), Transparenz (Art. 13), menschliche Aufsicht (Art. 14) sowie Genauigkeit, Robustheit und Cybersicherheit (Art. 15) (Europäische Union, 2024, S. 56–61). Diese Normen sind jedoch primär als juristisch-normative Pflichten formuliert; sie liefern keine technische Übersetzung in maschinenprüfbare Controls und keine durchgängige Evidenzkette für Audits.In der Trustworthy-AI-Literatur wird hervorgehoben, dass eine konsistente Abbildung von regulatorischen Anforderungen in operative, automatisiert verifizierbare Protokolle nicht existiert (Díaz-Rodríguez et al., 2023, S. 14), während die Governance-Forschung als deutlich unterentwickelt gegenüber der Geschwindigkeit der technologischen Entwicklung beschrieben wird (Taeihagh, 2025, S. 2, 7). Zudem erfordern die Dokumentations- und Evidenzanforderungen des AI Act — insbesondere Art. 11 und Art. 12 — eine Kombination rechtlicher und technischer Expertise, die in der Praxis selten in ausreichender Tiefe vorhanden ist und erheblichen Aufwandsdruck erzeugt (Sovrano et al., 2025, S. 1–2, 31). Compliance-Nachweisführung bleibt damit häufig reaktiv statt als kontinuierlicher, automatisierter Prozess entlang der Pipeline.Verschärfend kommt hinzu, dass sich AI-Act-Compliance in der Praxis mit weiteren Rechtsbereichen wie Datenschutz, Intellectual Property und Cybersicherheit verschränkt (Novelli et al., 2024, S. 2, 10, 14). Obwohl in DevSecOps-Kontexten Security-Controls bereits als automatisierte Practices in Pipelines etabliert sind — etwa Infrastructure-as-Code, automatisierte Tests und kontinuierliches Monitoring (Sinan et al., 2025, S. 1–6) —, adressieren diese Ansätze primär Infrastruktur- und allgemeine Sicherheitskontrollen. Ein vergleichbar systematischer Ansatz, der AI-Act-spezifische Pflichten wie Bias-Evaluation, Erklärbarkeit oder kontinuierliches Risikomonitoring konsistent in Policy-/Compliance-as-Code überführt und über den GenAI-Lifecycle orchestriert, ist in der bestehenden Literatur nicht dokumentiert.Synthese der Problemdimensionen und resultierende ForschungslückeDie drei Problemdimensionen beschreiben eine kumulative Lücke: Bestehende Operationalisierungsframeworks sind fragmentiert und decken den GenAI-Lebenszyklus nicht integriert ab (PD1). Formalisierte, automatisierbare Qualitätssicherungsmechanismen, die der stochastischen Natur generativer Systeme methodisch gerecht werden, fehlen (PD2). Und eine systematische Übersetzung der EU-AI-Act-Anforderungen in maschinenprüfbare Policies, skalierbare Evidenzerzeugung und durchgängige Compliance-Orchestrierung ist nicht dokumentiert (PD3).Diese dreifache Lücke — strukturell, methodisch, regulatorisch — begründet den Bedarf an einer integrierten Enterprise-Referenzarchitektur, die cloud-native GenAIOps-Praktiken mit automatisierten Quality Gates verbindet und die Anforderungen des EU AI Act als prüfbare Kontrollmechanismen in CI/CD/CT-Pipelines operationalisiert. Die Entwicklung dieser Architektur als generalisierbares, wiederverwendbares Artefakt im Sinne des Design Science Research nach Hevner et al. (2004) ist Gegenstand der vorliegenden Arbeit.1.3 Zielsetzung der Arbeit und LeitfragenZiel der vorliegenden Arbeit ist die Entwicklung einer modularen, cloud-nativen Enterprise-Referenzarchitektur für den produktiven Betrieb generativer KI-Systeme, die automatisierte Quality Gates als zentrale Steuerungsmechanismen entlang des End-to-End-Lebenszyklus integriert und die Anforderungen des EU AI Act systematisch operationalisiert. Die Architektur wird deployment-agnostisch konzipiert, d. h. auf Public Cloud, Sovereign Cloud und On-Premises-Umgebungen gleichermaßen anwendbar. Das Artefakt wird im Sinne des Design Science Research nach Hevner et al. (2004) als generalisierbares, wiederverwendbares Designwissen entwickelt — nicht als unternehmensspezifische Implementierung, sondern als übertragbare Referenzarchitektur mit expliziten Design-Prinzipien (DP-1 bis DP-n).Die Architektur umfasst drei integrierte Bestandteile: (1) ein Schichten- und Modulmodell, das die zentralen Funktionsbereiche cloud-nativer GenAIOps-Umgebungen strukturiert (u. a. Data, Model, Serving, Monitoring, Security, Governance, Platform), (2) ein Quality-Gate-Framework, das Kontrollpunkte entlang dreier Dimensionen — strategisch (Lifecycle-Governance), technisch (Modellqualität, Performance, Safety) und Compliance (EU-AI-Act-Anforderungen) — formalisiert, sowie (3) eine Einbettungslogik, die diese Gates als Policy-as-Code- und Compliance-as-Code-Mechanismen in CI/CD/CT-Pipelines verankert, einschließlich definierter Trigger, Prüfkriterien, Evidenzartefakte und Freigabeentscheidungen.Der Erreichungsgrad der Zielsetzung wird anhand einer vierstufigen Evaluation überprüft: einem Requirements-Coverage-Check, einem szenariobasierten Walkthrough am Anwendungsfall eines Ambient AI Scribe, einer prototypischen Implementierung (Proof of Concept) sowie leitfadengestützten Experteninterviews mit Domänenexperten aus Wissenschaft und Praxis. Die prototypische Implementierung fokussiert dabei auf drei der sechs Lebenszyklusphasen — Evaluation, Deployment und Production —, die exemplarisch auf Microsoft Azure instanziiert werden; die übrigen drei Phasen (Data, Model, Retire) werden konzeptionell abgedeckt.Aus der in Abschnitt 1.2 identifizierten Forschungslücke leiten sich folgende Forschungsfragen ab:Hauptforschungsfrage (HRQ): Wie muss eine cloud-native Enterprise-Referenzarchitektur gestaltet sein, damit automatisierte Quality Gates den End-to-End-Lebenszyklus generativer KI-Systeme steuern und die kontinuierliche Einhaltung der Anforderungen des EU AI Act systematisch unterstützen?RQ1: Welche technischen, organisatorischen und regulatorischen Anforderungen ergeben sich an den produktiven Betrieb generativer KI-Systeme in cloud-nativen Enterprise-Umgebungen unter Berücksichtigung des EU AI Act?RQ2: Welche Design-Prinzipien und Architekturbausteine sind erforderlich, um ein integriertes Quality-Gate-Framework entlang des GenAI-Lebenszyklus in eine cloud-native Betriebsumgebung einzubetten?RQ3: Inwieweit erfüllt die entwickelte Referenzarchitektur die abgeleiteten Anforderungen, und welche Stärken, Schwächen und Anpassungsbedarfe zeigen sich bei der prototypischen Anwendung und der Bewertung durch Domänenexperten?1.4 Methodik1.4.1 ForschungsmethodeDie vorliegende Arbeit folgt dem Design Science Research (DSR)-Paradigma, das auf die Konstruktion und Evaluation zweckgerichteter IT-Artefakte ausgerichtet ist (Hevner et al., 2004, S. 77). Das Forschungsziel ist die präskriptive Gestaltung einer neuartigen Architekturlösung — ein Artefakttyp, der sich der Kategorie „Model" im Sinne von March und Smith (1995) zuordnen lässt. Als Vorgehensmodell dient die Design Science Research Methodology (DSRM) nach Peffers et al. (2007) mit problemzentriertem Einstieg; zur Strukturierung wird ergänzend das DSR Grid nach vom Brocke und Maedche (2019) herangezogen. Die Wissensbasis (Rigor Cycle) wird durch eine strukturierte Literaturanalyse aufgebaut, der Relevance Cycle verankert die Arbeit im Enterprise-Kontext durch Ableitung eines Anforderungskatalogs (R1–Rn), und im Design Cycle werden daraus Design-Prinzipien (DP-1 bis DP-n) abgeleitet und iterativ in die Referenzarchitektur überführt. Die Evaluation erfolgt vierstufig wie in Abschnitt 1.3 beschrieben.1.4.2 Technische KonstruktionsmethodenNeben der Forschungsmethode erfordert die Artefaktkonstruktion die Festlegung technischer Konstruktionsmethoden, die bestimmen, mit welchen Entwurfsansätzen, Prinzipien und Werkzeugkategorien die Referenzarchitektur entwickelt wird. Diese lassen sich entlang von vier Säulen strukturieren, die jeweils eine zentrale Gestaltungsdimension des Artefakts adressieren. Die Referenzarchitektur wird dabei plattformagnostisch entworfen; für die prototypische Evaluation werden die generischen Werkzeugkategorien auf Microsoft Azure instanziiert.Die erste Säule umfasst den Architekturentwurf des Schichten- und Modulmodells. Methodisch wird ein Referenzarchitektur-Entwurf auf Basis cloud-nativer Architekturmuster verfolgt, der die Prinzipien Separation of Concerns, Modularität, Loose Coupling und Infrastructure-as-Code umsetzt. Auf generischer Ebene setzt dies einen Container Orchestrator, eine IaC-Engine und Konfigurationsmanagement voraus; im PoC werden diese durch Azure Kubernetes Service (AKS), Terraform/Bicep und Helm instanziiert.Die zweite Säule betrifft das Quality-Gate-Framework. Die Methodik kombiniert template-basierte Spezifikation mit deklarativer Regelspezifikation und adaptiert etablierte DevSecOps-Gate-Patterns für den GenAI-Kontext. Leitende Prinzipien sind Policy-as-Code, die Trennung von Policy und Enforcement sowie evidenzbasierte Entscheidungen entlang dreier Dimensionen (strategisch, technisch, Compliance). Jedes Gate wird durch ein einheitliches Template formalisiert: Trigger, Kriterien, Evidenzartefakte, Entscheidung, Verantwortlichkeit, Audit Trail und Waiver-Regelung. Generisch erfordert dies eine Policy Engine mit deklarativer Policy Language sowie ein GenAI-Evaluationsframework; im PoC kommen OPA/Rego (CNCF Graduated Project) und DeepEval zum Einsatz.Die dritte Säule adressiert die Pipeline-Integration. Methodisch werden CI/CD/CT-Pipelines als Pipeline-as-Code realisiert, ergänzt durch GitOps-Prinzipien (deklarativer Zielzustand, Git als Single Source of Truth) und Continuous Verification. Weitere Prinzipien umfassen Immutable Artifacts und Supply-Chain-Security (SLSA). Generisch setzt dies eine CI/CD-Plattform, einen GitOps-Operator und Artefakt-Signierung voraus; im PoC werden GitHub Actions, ArgoCD und Sigstore/Cosign eingesetzt.Die vierte Säule umfasst die Compliance-Operationalisierung. Methodisch werden die Anforderungen des EU AI Act (insb. Art. 9–15) durch ein Regulatory-to-Technical Mapping in prüfbare Controls überführt und als Compliance-as-Code implementiert. Leitende Prinzipien sind Auditierbarkeit-by-Design, Immutable Evidence und Continuous Compliance. Generisch erfordert dies eine Policy Engine für Compliance-Regeln und einen immutablen Evidence Store; im PoC werden OPA/Rego für AI-Act-Policies, Azure Blob Storage (immutable) und Azure Monitor eingesetzt.Das Zusammenspiel der vier Säulen erzeugt das zentrale Artefakt: Die Architektur (Säule 1) definiert die Struktur, das Gate-Framework (Säule 2) definiert die Kontrollmechanismen, die Pipeline-Integration (Säule 3) automatisiert deren Durchsetzung, und die Compliance-Operationalisierung (Säule 4) stellt die regulatorische Nachweisbarkeit sicher. 2.1 MotivationDie Themenwahl ist sowohl durch die berufliche Spezialisierung als auch durch ein weiterführendes Forschungsinteresse motiviert. Zurzeit absolviere ich parallel zur Masterarbeit eine zertifizierte Vollzeit-Weiterbildung zum Cloud Architect (810 UE, AZAV-zertifiziert) mit Schwerpunkt auf Kubernetes, Infrastructure-as-Code und CI/CD-Pipelines, die die technische Basis der Arbeit unmittelbar ergänzt. Im Masterstudium bildete Künstliche Intelligenz das Vertiefungsfach (Note: 1,0), ergänzt durch Praxisprojekte im Bereich KI-gestütztes Requirements Engineering und cloud-native Architekturentwicklung (jeweils Note: 1,0). Die berufliche Erfahrung im Automotive-Sektor — unter anderem als Projektleiter bei Mercedes-AMG und als strategische Produktmanager bei der Mercedes-Benz AG mit Verantwortung für Schnittstellenmanagement, Datenintegration und Governance-Strukturen — hat den Bedarf an integrierten, regulatorisch konformen Betriebsarchitekturen für KI-Systeme aus der Praxis heraus sichtbar gemacht.Über das unmittelbare Berufsziel als AI & Cloud Solution Architect hinaus verfolge ich das Ziel einer Promotion im Bereich KI- und Innovationsmanagement. Die vorliegende Arbeit soll dafür eine methodisch und inhaltlich tragfähige Grundlage schaffen. Darüber hinaus ist die Überzeugung leitend, dass die technologische und regulatorische Souveränität Europas — insbesondere im Bereich Cloud-Infrastruktur und KI-Governance — eine eigenständige architektonische Antwort erfordert, zu der diese Arbeit einen konzeptionellen Beitrag leisten möchte.2.2 UnternehmensakzeptanzVorerst wird die Arbeit als rein akademisches Forschungsprojekt ohne festen Praxispartner durchgeführt. Dies ist im Rahmen einer Design-Science-Research-Arbeit methodisch unproblematisch, da das zentrale Artefakt — eine generalisierbare Referenzarchitektur — nicht an einen spezifischen Unternehmenskontext gebunden ist, sondern als übertragbares Designwissen konzipiert wird (Hevner et al., 2004, S. 80). Die Evaluation stützt sich auf leitfadengestützte Experteninterviews mit Domänenexperten aus Wissenschaft und Praxis, deren Zugang über das akademische und berufliche Netzwerk von mir gesichert ist. Für die prototypische Implementierung (PoC) stehen studentische Cloud-Ressourcen (Microsoft Azure) sowie Open-Source-Tooling zur Verfügung. Eine optionale Kooperation mit einem Startup zur Validierung des Ambient-AI-Scribe-Szenarios (oder eines ähnlichen Use Case) wird derzeit geprüft, ist für die Durchführbarkeit der Arbeit jedoch nicht erforderlich.2.3 ZeitplanPhaseZeitraumAktivitätenPhase 1: Grundlagen & AnforderungenMonat 1Literaturanalyse (Kap. 2), Anforderungsableitung (Kap. 4), Traceability-MatrixPhase 2: ArchitekturentwicklungMonat 1–3Design-Prinzipien, Schichtenmodell, Quality-Gate-Framework, Pipeline-Integration (Kap. 5)Phase 3: EvaluationMonat 4-5Coverage-Check, Walkthrough, PoC-Implementierung, Experteninterviews (Kap. 6)Phase 4: Dokumentation & AbschlussMonat 5Diskussion (Kap. 7), Fazit (Kap. 8), Revision, Abgabe2.4 Grobgliederung 1 Einleitung1.1 Problemstellung und Relevanz1.2 Zielsetzung der Arbeit1.3 Forschungsfragen1.4 Aufbau der Arbeit1.5 PoC-Scope (EVAL→DEPLOY→PROD) explizit erwähnen2 Theoretische Grundlagen und Stand der Forschung → Rigor Cycle2.1 Begriffsabgrenzung und Systemkontext (LLM, GenAI-System, RAG, LLMOps, Quality Gate, Policy-as-Code).2.2 Generative KI und LLMs2.3 Cloud-native: Fokus auf Plattformfähigkeiten (Identity, Network, Observability, Policy)2.4 Operationalisierung     2.4.1 Von MLOps zu LLMOps: Operationalisierung von KI-Systemen    2.4.2 Der EU AI Act: Regulatorischer Rahmen und technisch-organisatorische Anforderungen2.5 Quality Gates: Definition, Konzepte und Einordnung2.6 Compliance-as-Code und Policy-as-Code2.7 Synthese: Forschungslücke und Handlungsbedarf3 Forschungsdesign und Methodik  3.1 Design Science Research als Forschungsrahmen  3.2 Vorgehensmodell (DSRM nach Peffers)  3.3 Artefaktdefinition, Scope & Abgrenzung, Instanziierungsstrategie  3.4 Technische Konstruktionsmethoden      3.4.1 Architekturentwurf (Cloud-native Patterns, IaC)      3.4.2 Quality-Gate-Framework (Policy-as-Code, Gate-Template)      3.4.3 Pipeline-Integration (CI/CD/CT, GitOps)      3.4.4 Compliance-Operationalisierung (Regulatory-to-Technical Mapping)  3.5 Anforderungserhebung und -analyse  3.6 Traceability-Ansatz  3.7 Evaluationsdesign (Kriterien + Methodenmix)  3.8 Interviewdesign & Auswertung4 Anforderungsanalyse → Relevance Cycle / beantwortet RQ14.1 Ableitungslogik für testbare Anforderungen (MUSS/SOLL, Format R1–Rn, Evidenz, Lifecycle-Phase, Bezug EU AI ACT)4.2 Technische Anforderungen4.3 Organisatorische Anforderungen4.4 Regulatorische Anforderungen4.5 Konsolidierter Anforderungskatalog5 Entwicklung der Referenzarchitektur → Design Cycle / beantwortet RQ25.1 Design-Prinzipien (DP-1..DP-n, begründet aus R1–Rn)5.2 Architekturübersicht: Schichtenmodell, Module, Schnittstellen, Verantwortlichkeiten5.3 Quality-Gate-Framework5.4 Gate-Template (Trigger, Evidence, Policy, Decision, Owner, Audit Trail, Waiver) - 5.4.1 Strategische Gates (Lifecycle Governance)- 5.4.2 Technische Gates (Qualität/Performance/Safety-Evals, Monitoring)- 5.4.3 Compliance Gates (EU AI Act → Policy-as-Code + Evidence)- 5.4.4 Gate-Katalog (Übersichtstabelle über alle Gates) 5.5 Integration in CI/CD/CT-Pipelines (Policy Engine, Evidence Store, Automation Points)6 Evaluation → beantwortet RQ36.1 Evaluationskriterien (z. B. Coverage, Automatisierbarkeit, Auditierbarkeit,  Praxistauglichkeit)6.2 Requirements-Coverage-Check6.3 Szenario-basierter Walkthrough: Ambient AI Scribe6.4 PoC-Scope EVAL→DEPLOY→PROD + Azure sichtbar-  6.4.1 Technisches Quality Gate-  6.4.2 Compliance Gate (Policy-as-Code)6.5 Experteninterviews: Durchführung und Ergebnisse6.6 Synthese der Evaluationsergebnisse7 Diskussion7.1 Beantwortung der Forschungsfragen7.2 Wissenschaftlicher Beitrag und Einordnung - 7.2.1 Design Knowledge: übertragbare Muster/Prinzipien (DSR-Beitrag)7.3 Implikationen für die Praxis7.4 Limitationen7.5 Weiterer Forschungsbedarf8 Fazit und Ausblick
Literaturverzeichnis (Exposé)Blancato, F. G. (2024). The cloud sovereignty nexus: How the European Union seeks to reverse strategic dependencies in its digital ecosystem. Policy & Internet, 16(1), 12–30. https://doi.org/10.1002/poi3.358Díaz-De-Arcaya, J., Torre-Bastida, A. I., Zárate, G., Miñón, R., & Almeida, A. (2024). LLMOps: A State of the Art and Comprehensive Overview. arXiv preprint, arXiv:2407.11657. https://doi.org/10.15223/policy-029 Díaz-Rodríguez, N., Del Ser, J., Coeckelbergh, M., López de Prado, M., Herrera-Viedma, E., & Herrera, F. (2023). Connecting the dots in trustworthy Artificial Intelligence: From AI principles, ethics, and key requirements to responsible AI systems and regulation. Information Fusion, 99, 101896. https://doi.org/10.1016/j.inffus.2023.101896Europäische Union. (2024). Verordnung (EU) 2024/1689 des Europäischen Parlaments und des Rates vom 13. Juni 2024 zur Festlegung harmonisierter Vorschriften für künstliche Intelligenz (Verordnung über künstliche Intelligenz). Amtsblatt der Europäischen Union, L 2024/1689. https://eur-lex.europa.eu/eli/reg/2024/1689/ojHevner, A. R., March, S. T., Park, J., & Ram, S. (2004). Design science in information systems research. MIS Quarterly, 28(1), 75–105. https://doi.org/10.2307/25148625Joshi, S. (2025). LLMOps, AgentOps, and MLOps for Generative AI: A Comprehensive Review. International Journal of Computer Applications Technology and Research. https://doi.org/10.7753/IJCATR1407.1001Kreuzberger, D., Kühl, N., & Hirschl, S. (2023). Machine Learning Operations (MLOps): Overview, Definition, and Architecture. IEEE Access, 11, 31866–31879. https://doi.org/10.1109/ACCESS.2023.3262138March, S. T., & Smith, G. F. (1995). Design and natural science research on information technology. Decision Support Systems, 15(4), 251–266. https://doi.org/10.1016/0167-9236(94)00041-2Novelli, C., Casolari, F., Hacker, P., Spedicato, G., & Floridi, L. (2024). Generative AI in EU law: Liability, privacy, intellectual property, and cybersecurity. Computer Law & Security Review, 55, 106066. https://doi.org/10.1016/j.clsr.2024.106066Pahune, S., & Akhtar, Z. (2025). Transitioning from MLOps to LLMOps: Navigating the Unique Challenges of Large Language Models. Information, 16(2), 87. https://doi.org/10.3390/info16020087Peffers, K., Tuunanen, T., Rothenberger, M. A., & Chatterjee, S. (2007). A design science research methodology for information systems research. Journal of Management Information Systems, 24(3), 45–77. https://doi.org/10.2753/MIS0742-1222240302Polyakovska, K. (2025). LLMOps: Adapting MLOps Strategies for Large Language Model Lifecycle Management. arXiv preprint, https://doi.org/10.36910/6775-2524-0560-2025-58-14Sinan, M., Shahin, M., & Gondal, I. (2025). Integrating Security Controls in DevSecOps: Challenges, Solutions, and Future Research Directions. Journal of Software: Evolution and Process, 37, e70029. https://doi.org/10.1002/smr.70029Sinha, A., Arora, A., & Gupta, A. (2024). Building AI Agents: Challenges in LLM Productionization. https://doi.org/10.15223/policy-029Sovrano, F., Hine, E., Anzolut, S., & Bacchelli, A. (2025). Simplifying software compliance: AI technologies in drafting technical documentation for the AI Act. Empirical Software Engineering, 30, 91. https://doi.org/10.1109/ICECET61485.2024.10698359Stone, J., Patel, R., Ghiasi, F., Mittal, S., & Rahimi, S. (2025). Navigating MLOps: Insights into Maturity, Lifecycle, Tools, and Careers. 2025 IEEE Conference on Artificial Intelligence (CAI), 643–650. https://doi.org/10.1109/CAI64502.2025.00118Taeihagh, A. (2025). Governance of Generative AI. Policy and Society, 44(1), 1–22. https://doi.org/10.1093/polsoc/puaf001Tantithamthavorn, C. K., Palomba, F., Khomh, F., & Chua, J. J. (2025). MLOps, LLMOps, FMOps, and Beyond. IEEE Software, 42(1), 26–32. https://doi.org/10.1109/MS.2024.3477014vom Brocke, J., & Maedche, A. (2019). The DSR grid: Six core dimensions for effectively planning and communicating design science research projects. Electronic Markets, 29(3), 379–385. https://doi.org/10.1007/s12525-019-00358-7Wang, K., Zhang, G., Zhou, Z., Wu, J., Yu, M., Zhao, S., Yin, C., Fu, J., Yan, Y., Luo, H., Lin, L., Xu, Z., Lu, H., Cao, X., Zhou, X., Jin, W., Meng, F., Xu, S., Mao, J., … Liu, Y. (2025). A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment (Version 4). arXiv. https://doi.org/10.48550/ARXIV.2504.15585Xu, X., Weytjens, H., Zhang, D., Lu, Q., Weber, I., & Zhu, L. (2025). RAGOps: Operating and Managing Retrieval-Augmented Generation Pipelines (Version 1). arXiv. https://doi.org/10.48550/ARXIV.2506.034011